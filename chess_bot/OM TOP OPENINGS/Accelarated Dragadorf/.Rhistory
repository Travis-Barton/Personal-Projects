add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/ae/db')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/ae/common')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/inrush')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/i_wf_channel_switching')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/e_wf_features_wt')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/demo')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/db')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/test')
add_package_to_sys_path(PROJECTS_ROOT, 'Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/tools')
import get_wf_by_ids as gwi
from detect_peaks import detect_peaks
from pq_utils import check_lol, get_windowed_rms_waveform_centered, get_frequency_content
from feeder_view_plots import get_feeder_view_plot
add_package_to_sys_path(PROJECTS_ROOT, '/Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/travis_analysis')
add_package_to_sys_path(PROJECTS_ROOT, '/Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/jl')
from feeder_view_plots import get_feeder_view_plot
add_package_to_sys_path(PROJECTS_ROOT, '/Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/jl_analysis')
from feeder_view_plots import get_feeder_view_plot
add_package_to_sys_path(PROJECTS_ROOT, '/Users/tbarton/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/i_wf_hiamp_peak_features')
from feeder_view_plots import get_feeder_view_plot
from matplotlib.pyplot import plot
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import Counter
from statistics import mode
import os
import scipy.stats as stats
def codes_done(title = 'Hey Dude', msg = 'Your code is done'):
os.system("osascript -e 'display notification \"{}\" with title \"{}\"'".format(msg, title))
reticulate::repl_python()
reticulate::repl_python()
db_val = db_val.loc[db_val['ratio_total_fund_min'] < 1.1, :]
#db_val.to_csv('info_waves.csv')
dat_val = pd.DataFrame()
dat_val = dat_val.append(db_val['e_wf_raw_bytes'].apply(np.frombuffer, dtype = '<i4').apply(pd.Series))
print('half way there')
dat_val = dat_val.multiply(db_val['e_post_scale'], axis = 0)
#dat_val = dat_val.apply(lambda row: get_windowed_rms_waveform_centered(130, np.array(row)), axis = 1).apply(pd.Series)
raw_wf_val = dat_val.copy()
dat_val = dat_val.apply(get_frequency_content, frequency = 60, sample_rate = 7812.5, axis = 1).apply(pd.Series).iloc[:,0].apply(pd.Series)
#dat_val.to_csv('actual_waves_rms_i.csv')
dat_val.index = db_val['disturbance_id']
raw_wf_val.index = db_val['disturbance_id']
#get_frequency_content(wf, frequency, sample_rate, support=3, freq_bandwidth=2, freq_center=1)
# Lets get back to the problem at hand. Can we manually label the data of choice.
codes_done()
#low_data_index = np.where((db_val['loss_of_load'] < 2250) & (db_val['loss_of_load'] > 750))[0] #fix this range to be between 750 and 1500 tomorrow
#low_data = dat.iloc[np.r_[low_data_index], :]
#values = [db_val['loss_of_load'][i] for i in low_data_index]
#values = np.array(values)[0]
#
#
#
## Another round! Hopefully these will be a little clearer. Lets do the range, [5300, 7700]
#
#
#mid_data_index = np.where((temp_db2['loss_of_load'] < 7700) & (temp_db2['loss_of_load'] > 5300)) #fix this range to be between 750 and 1500 tomorrow
#mid_data = dat.iloc[np.r_[mid_data_index], :]
#values2 = [temp_db2['loss_of_load'][i] for i in mid_data_index]
#values2 = np.array(values2)[0]
#mid_data_ids = pd.DataFrame([temp_db2['waveform_id'][i] for i in mid_data_index]).T
#
#
#
#
#
## Third rounds the charm! Last areas seem fairly cut and dry, but I'd like to see more uniformity
## looking at range [8300, 10000]
#
#high_data_index = np.where((temp_db2['loss_of_load'] < 10000) & (temp_db2['loss_of_load'] > 8300)) #fix this range to be between 750 and 1500 tomorrow
#high_data = dat.iloc[np.r_[high_data_index], :]
#values3 = [temp_db2['loss_of_load'][i] for i in high_data_index]
#values3 = np.array(values3)[0]
#
#
### Using the Dynamic time warping distance metric, I am going to cluser the
### mid data and check its validity with the silhouette distance and with
### my active labeling algorithm.
def normalize(vec):
def calc(x, mi, ma):
return((x-mi)/(ma-mi))
mi = np.nanmin(vec)
ma = np.nanmax(vec)
return(np.apply_along_axis(calc, 0, vec, mi, ma))
def standardize(vec):
return(vec/np.nanmax(vec))
codes_done()
quit
reticulate::source_python('~/Documents/GitHub/advanced-projects/python/projects/wfl/analysis/travis_analysis/clustering_sandbox.py')
from multiprocessing import  Pool
def par_df(df, func, n_cores=4):
df_split = np.array_split(df, n_cores)
pool = Pool(n_cores)
df = pd.concat(pool.map(func, df_split))
pool.close()
pool.join()
return df
sql2 = '''
SElECT
iwf.*, dwr.e_wf_raw_bytes, b.e_post_scale, b.e_post_offset, dwr.i_wf_raw_bytes, b.i_post_scale, b.i_post_offset, dq.ratio_total_fund_max, dq.ratio_total_fund_min
FROM
i_wf_preprocessing_features iwf
JOIN disturbance_waveforms_raw dwr
ON dwr.waveform_id = iwf.waveform_id
JOIN disturbance_mm3_basics b
on iwf.disturbance_id = b.disturbance_id
JOIN e_wf_features_dq dq
ON iwf.disturbance_id = dq.disturbance_id
WHERE
iwf.loss_of_load != 0
AND
iwf.pre_i_rms_wf > 5
AND
iwf.post_i_rms_wf < 5
limit 10000
'''
sql_val = '''
SElECT
iwf.*, dwr.e_wf_raw_bytes, b.e_post_scale, b.e_post_offset, dwr.i_wf_raw_bytes, b.i_post_scale, b.i_post_offset, dq.ratio_total_fund_max, dq.ratio_total_fund_min
FROM
i_wf_preprocessing_features iwf
JOIN disturbance_waveforms_raw dwr
ON dwr.waveform_id = iwf.waveform_id
JOIN disturbance_mm3_basics b
on iwf.disturbance_id = b.disturbance_id
JOIN e_wf_features_dq dq
ON iwf.disturbance_id = dq.disturbance_id
WHERE
iwf.loss_of_load != 0
AND
iwf.pre_i_rms_wf > 5
AND
iwf.post_i_rms_wf < 5
ORDER BY RAND()
limit 1000
'''
temp_db2 = pd.read_sql(sql2, dbc.db_connect(dbc.wfl_db_config))
temp_db2 = temp_db2.loc[temp_db2['ratio_total_fund_min'] < 1.1, :]
#temp_db2.to_csv('info_waves.csv')
dat = pd.DataFrame()
dat = dat.append(temp_db2['e_wf_raw_bytes'].apply(np.frombuffer, dtype = '<i4').apply(pd.Series))
dat = dat.multiply(temp_db2['e_post_scale'], axis = 0)
#dat = dat.apply(lambda row: get_windowed_rms_waveform_centered(130, np.array(row)), axis = 1).apply(pd.Series)
raw_wf = dat.copy()
dat = dat.apply(get_frequency_content, frequency = 60, sample_rate = 7812.5, axis = 1).apply(pd.Series).iloc[:,0].apply(pd.Series)
#dat.to_csv('actual_waves_rms_i.csv')
dat.index = temp_db2['disturbance_id']
raw_wf.index = temp_db2['disturbance_id']
#get_frequency_content(wf, frequency, sample_rate, support=3, freq_bandwidth=2, freq_center=1)
# Lets get back to the problem at hand. Can we manually label the data of choice.
codes_done()
db_val = pd.read_sql(sql_val, dbc.db_connect(dbc.wfl_db_config))
db_val = db_val.loc[db_val['ratio_total_fund_min'] < 1.1, :]
#db_val.to_csv('info_waves.csv')
dat_val = pd.DataFrame()
dat_val = dat_val.append(db_val['e_wf_raw_bytes'].apply(np.frombuffer, dtype = '<i4').apply(pd.Series))
print('half way there')
dat_val = dat_val.multiply(db_val['e_post_scale'], axis = 0)
#dat_val = dat_val.apply(lambda row: get_windowed_rms_waveform_centered(130, np.array(row)), axis = 1).apply(pd.Series)
raw_wf_val = dat_val.copy()
dat_val = dat_val.apply(get_frequency_content, frequency = 60, sample_rate = 7812.5, axis = 1).apply(pd.Series).iloc[:,0].apply(pd.Series)
#dat_val.to_csv('actual_waves_rms_i.csv')
dat_val.index = db_val['disturbance_id']
raw_wf_val.index = db_val['disturbance_id']
#get_frequency_content(wf, frequency, sample_rate, support=3, freq_bandwidth=2, freq_center=1)
# Lets get back to the problem at hand. Can we manually label the data of choice.
codes_done()
#low_data_index = np.where((db_val['loss_of_load'] < 2250) & (db_val['loss_of_load'] > 750))[0] #fix this range to be between 750 and 1500 tomorrow
#low_data = dat.iloc[np.r_[low_data_index], :]
#values = [db_val['loss_of_load'][i] for i in low_data_index]
#values = np.array(values)[0]
#
#
#
## Another round! Hopefully these will be a little clearer. Lets do the range, [5300, 7700]
#
#
#mid_data_index = np.where((temp_db2['loss_of_load'] < 7700) & (temp_db2['loss_of_load'] > 5300)) #fix this range to be between 750 and 1500 tomorrow
#mid_data = dat.iloc[np.r_[mid_data_index], :]
#values2 = [temp_db2['loss_of_load'][i] for i in mid_data_index]
#values2 = np.array(values2)[0]
#mid_data_ids = pd.DataFrame([temp_db2['waveform_id'][i] for i in mid_data_index]).T
#
#
#
#
#
## Third rounds the charm! Last areas seem fairly cut and dry, but I'd like to see more uniformity
## looking at range [8300, 10000]
#
#high_data_index = np.where((temp_db2['loss_of_load'] < 10000) & (temp_db2['loss_of_load'] > 8300)) #fix this range to be between 750 and 1500 tomorrow
#high_data = dat.iloc[np.r_[high_data_index], :]
#values3 = [temp_db2['loss_of_load'][i] for i in high_data_index]
#values3 = np.array(values3)[0]
#
#
### Using the Dynamic time warping distance metric, I am going to cluser the
### mid data and check its validity with the silhouette distance and with
### my active labeling algorithm.
def normalize(vec):
def calc(x, mi, ma):
return((x-mi)/(ma-mi))
mi = np.nanmin(vec)
ma = np.nanmax(vec)
return(np.apply_along_axis(calc, 0, vec, mi, ma))
def standardize(vec):
return(vec/np.nanmax(vec))
codes_done()
reticulate::repl_python()
db_val = db_val.loc[db_val['ratio_total_fund_min'] < 1.1, :]
#db_val.to_csv('info_waves.csv')
dat_val = pd.DataFrame()
dat_val = dat_val.append(db_val['e_wf_raw_bytes'].apply(np.frombuffer, dtype = '<i4').apply(pd.Series))
print('half way there')
dat_val = dat_val.multiply(db_val['e_post_scale'], axis = 0)
#dat_val = dat_val.apply(lambda row: get_windowed_rms_waveform_centered(130, np.array(row)), axis = 1).apply(pd.Series)
raw_wf_val = dat_val.copy()
dat_val = dat_val.apply(get_frequency_content, frequency = 60, sample_rate = 7812.5, axis = 1).apply(pd.Series).iloc[:,0].apply(pd.Series)
#dat_val.to_csv('actual_waves_rms_i.csv')
dat_val.index = db_val['disturbance_id']
raw_wf_val.index = db_val['disturbance_id']
#get_frequency_content(wf, frequency, sample_rate, support=3, freq_bandwidth=2, freq_center=1)
# Lets get back to the problem at hand. Can we manually label the data of choice.
codes_done()
#low_data_index = np.where((db_val['loss_of_load'] < 2250) & (db_val['loss_of_load'] > 750))[0] #fix this range to be between 750 and 1500 tomorrow
#low_data = dat.iloc[np.r_[low_data_index], :]
#values = [db_val['loss_of_load'][i] for i in low_data_index]
#values = np.array(values)[0]
#
#
#
## Another round! Hopefully these will be a little clearer. Lets do the range, [5300, 7700]
#
#
#mid_data_index = np.where((temp_db2['loss_of_load'] < 7700) & (temp_db2['loss_of_load'] > 5300)) #fix this range to be between 750 and 1500 tomorrow
#mid_data = dat.iloc[np.r_[mid_data_index], :]
#values2 = [temp_db2['loss_of_load'][i] for i in mid_data_index]
#values2 = np.array(values2)[0]
#mid_data_ids = pd.DataFrame([temp_db2['waveform_id'][i] for i in mid_data_index]).T
#
#
#
#
#
## Third rounds the charm! Last areas seem fairly cut and dry, but I'd like to see more uniformity
## looking at range [8300, 10000]
#
#high_data_index = np.where((temp_db2['loss_of_load'] < 10000) & (temp_db2['loss_of_load'] > 8300)) #fix this range to be between 750 and 1500 tomorrow
#high_data = dat.iloc[np.r_[high_data_index], :]
#values3 = [temp_db2['loss_of_load'][i] for i in high_data_index]
#values3 = np.array(values3)[0]
#
#
### Using the Dynamic time warping distance metric, I am going to cluser the
### mid data and check its validity with the silhouette distance and with
### my active labeling algorithm.
def normalize(vec):
def calc(x, mi, ma):
return((x-mi)/(ma-mi))
mi = np.nanmin(vec)
ma = np.nanmax(vec)
return(np.apply_along_axis(calc, 0, vec, mi, ma))
def standardize(vec):
return(vec/np.nanmax(vec))
codes_done()
reticulate::repl_python()
library(tuneR)
install.packages("Rwave")
library(Rwave)
x <- 1:512
chirp <- sin(2*pi * (x + 0.002 * (x-256)^2 ) / 16)
retChirp <- cwt(chirp, noctave=5, nvoice=12)
retChirp <- cwt(chirp, noctave=5, nvoice=12, plot = FALSE)
retChirp
dim(retChirp)
View(retChirp)
View(retChirp)
?cwt
cwt
retChirp
myseq=data.frame(myseq=seq(1,9,by=1/7))
myseq$channel=seq(1,length(myseq$myseq),by=1)
myseq$Hz=round(7812.5/(2^myseq$myseq),2)
mychannels=c(16,21,27,32,43)
load("~/Downloads/example_input_1.RData")
View(my_data_frame)
my_wave_info = my_data_frame_described_above
my_wave_info = my_data_frame
### filters for labeling purpose
filters=c(900,540,300,180,60)
current_metadata=cwt(unlist(fromJSON(my_wave_info$IT))*my_wave_info$post_scale_i,
noctave=8, nvoice=7, w0=2 * pi, twoD=TRUE, plot=FALSE)
library(RJSONIO)
library(Rwave)
current_metadata=cwt(unlist(fromJSON(my_wave_info$IT))*my_wave_info$post_scale_i,
noctave=8, nvoice=7, w0=2 * pi, twoD=TRUE, plot=FALSE)
current_metadata
View(current_metadata)
my_wave_info$waveform_id
length(fromJSON(my_wave_info$IT))*my_wave_info$post_scale_i)
length(fromJSON(my_wave_info$IT)*my_wave_info$post_scale_i)
length(unlist(fromJSON(my_wave_info$IT))*my_wave_info$post_scale_i)
current_metadata
dim(current_metadata)
x <- 1:512
chirp <- sin(2*pi * (x + 0.002 * (x-256)^2 ) / 16)
retChirp <- cwt(chirp, noctave=5, nvoice=12, plot = FALSE)
current_metadata=cwt(unlist(fromJSON(my_wave_info$IT))*my_wave_info$post_scale_i,
noctave=8, nvoice=7, w0=2 * pi, twoD=TRUE, plot=FALSE)
dim(current_metadata)
current_metadata[1, 1]
36.75*2000
total_income = 36.75*2000
monthly_pre_tax = total/12
monthly_pre_tax = total_income/12
monthly_pre_tax
fidelity_cut = monthly_pre_tax*.12
fidelity_cut
fidelity_cut = monthly_pre_tax*.12
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
monthly_post_tax
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
take_home_post_nessesities = take_home - rent - utilities - gas
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*.8
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
fidelity_cut
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*.08
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*.05
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*0
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*.05
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*.08
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
total_income = 36.75*2000
monthly_pre_tax = total_income/12
fidelity_cut = monthly_pre_tax*.12
fidelity_cut
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
giving = 250
roth_ira_contributions = 611
take_home = monthly_post_tax - giving - roth_ira_contributions
take_home
rent = 1100
utilities = 120
gas = 100
take_home_post_nessesities = take_home - rent - utilities - gas
take_home_post_nessesities
roth_ira_contributions/monthly_post_tax
fidelity_cut = monthly_pre_tax*.05
roth_ira_contributions/monthly_post_tax
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
roth_ira_contributions/monthly_post_tax
fidelity_cut = monthly_pre_tax*0
monthly_post_tax = (monthly_pre_tax-fidelity_cut)*.66
roth_ira_contributions/monthly_post_tax
512+161+129
512/(512+161+129)
fidelity_cut
fidelity_cut = monthly_pre_tax*.12
fidelity_cut
install.packages("cranlogs")
#install.packages("cranlogs")
library(cranlogs)
library(ggplot2)
cran_downloads(packages="LilRhino", when="last-week")
LR <- cran_downloads(packages="LilRhino", from = "2014-11-09", to = Sys.Date()-1)
sum(LR[,2])
gr0 <- ggplot(LR, aes(LR$date, LR$count)) +
geom_line(colour = "red",size=1)
gr0 + xlab("Time") + ylab("Nr. of downloads") +
labs(title = paste0("LilRhino daily downloads ", Sys.Date()-1))
LR <- cran_downloads(packages="LilRhino", from = "2019-01-01", to = Sys.Date()-1)
sum(LR[,2])
gr0 <- ggplot(LR, aes(LR$date, LR$count)) +
geom_line(colour = "red",size=1)
gr0 + xlab("Time") + ylab("Nr. of downloads") +
labs(title = paste0("LilRhino daily downloads ", Sys.Date()-1))
cumulative <- cumsum(LR[,2])
mls2 <- cbind(LR,cumulative)
#Plot
gr1 <- ggplot(mls2, aes(mls2$date, mls2$cumulative)) +
geom_line(colour = "blue",size=1)
gr1 + xlab("Time") + ylab("Nr. of downloads") +
labs(title = paste0("MetaLandSim cumulative downloads until ", Sys.Date()-1))
sum(LR[,2])
# thought experiment
vec = [0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1]
# thought experiment
vec = {0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1}
# thought experiment
vec = c(0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1)
cumsum(vec)
library(rwave)
library(Rwave)
?cwt
############ calculate "mychannels" to know what exactly we need to grab from cwt analysis below
myseq=data.frame(myseq=seq(1,9,by=1/7))
myseq$channel=seq(1,length(myseq$myseq),by=1)
myseq$Hz=round(7812.5/(2^myseq$myseq),2)
mychannels=c(16,21,27,32,43)
View(myseq)
setwd("~/Downloads/OM TOP OPENINGS/Accelarated Dragadorf")
library(bigchess)
chessfile = read.pgn('AcceleratedDragadorf.pgn',
add.tags = c('ECO', 'WhiteElo', 'BlackElo', 'PlyCount', 'EventDate',
'EventType', 'EventRounds', 'EventCountry', 'Source',
'SourceDate', 'WhiteTeamCountry', 'BlackTeamCountry',
'FEN'))
library(bigchess)
chessfile = read.pgn('AcceleratedDragadorf.pgn',
add.tags = c('ECO', 'WhiteElo', 'BlackElo', 'PlyCount', 'EventDate',
'EventType', 'EventRounds', 'EventCountry', 'Source',
'SourceDate', 'WhiteTeamCountry', 'BlackTeamCountry',
'FEN', 'BlackTeam', 'WhiteTeam'), big.mode = TRUE,
ignore.other.games=TRUE)
chessfile = read.pgn('AcceleratedDragadorf.pgn',
add.tags = c('ECO', 'WhiteElo', 'BlackElo', 'PlyCount', 'EventDate',
'EventType', 'EventRounds', 'EventCountry', 'Source',
'SourceDate', 'WhiteTeamCountry', 'BlackTeamCountry',
'FEN', 'BlackTeam', 'WhiteTeam'))
chessfile = read.pgn.ff('AcceleratedDragadorf.pgn',
add.tags = c('ECO', 'WhiteElo', 'BlackElo', 'PlyCount', 'EventDate',
'EventType', 'EventRounds', 'EventCountry', 'Source',
'SourceDate', 'WhiteTeamCountry', 'BlackTeamCountry',
'FEN', 'BlackTeam', 'WhiteTeam'))
LilRhino::codes_done('chess code complete')
LilRhino::Codes_done()
